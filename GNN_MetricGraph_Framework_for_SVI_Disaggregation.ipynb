{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMnrWzoPRQkBRUps+N6fWDE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JamionW/GRANITE/blob/main/GNN_MetricGraph_Framework_for_SVI_Disaggregation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GNN-MetricGraph Framework for SVI Disaggregation\n",
        "\n",
        "### This notebook demonstrates the integration of Graph Neural Networks with\n",
        "### MetricGraph's Whittle-Matérn framework for disaggregating Social Vulnerability\n",
        "### Index (SVI) from census tract level to address level."
      ],
      "metadata": {
        "id": "5axhuE9YI-XN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Determine the PyTorch and CUDA versions to get the correct installation command\n",
        "torch_version = torch.__version__\n",
        "cuda_available = torch.cuda.is_available()\n",
        "\n",
        "print(f\"PyTorch version: {torch_version}\")\n",
        "print(f\"CUDA available: {cuda_available}\")\n",
        "\n",
        "# Base installation command\n",
        "install_command = \"pip install torch_geometric\"\n",
        "\n",
        "if cuda_available:\n",
        "    # Get CUDA version (simplified way, can be more precise)\n",
        "    # Find the CUDA version used by PyTorch\n",
        "    cuda_version = torch.version.cuda\n",
        "    if cuda_version:\n",
        "        # Construct command based on CUDA version\n",
        "        # Example for CUDA 11.8. Replace with the actual command for your version if needed.\n",
        "        # Refer to https://pytorch-geometric.readthedocs.io/en/latest/install/installation.html\n",
        "        # NOTE: This is a common case for Colab, adjust if your environment differs\n",
        "        if 'cu118' in torch_version:\n",
        "             install_command += \" torch-scatter torch-sparse -f https://data.pyg.org/whl/torch-2.2.html\"\n",
        "        elif 'cu121' in torch_version:\n",
        "             install_command += \" torch-scatter torch-sparse -f https://data.pyg.org/whl/torch-2.2.html\" # Adjust URL based on your PyTorch version\n",
        "        else:\n",
        "             # Fallback or specific version required\n",
        "             print(\"Could not automatically determine the correct torch_geometric installation for your CUDA version.\")\n",
        "             print(\"Please refer to https://pytorch-geometric.readthedocs.io/en/latest/install/installation.html\")\n",
        "             install_command = \"# Please manually enter the correct installation command here.\"\n",
        "    else:\n",
        "         print(\"CUDA is available, but could not determine CUDA version from torch.version.cuda.\")\n",
        "         print(\"Please refer to https://pytorch-geometric.readthedocs.io/en/latest/install/installation.html and install manually.\")\n",
        "         install_command = \"# Please manually enter the correct installation command here.\"\n",
        "else:\n",
        "    # CPU only\n",
        "    install_command += \" torch-scatter torch-sparse\"\n",
        "\n",
        "\n",
        "print(f\"\\nSuggested installation command:\\n{install_command}\")\n",
        "print(\"\\nExecuting installation...\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H_BUJuOnL6n8",
        "outputId": "7e686a39-8089-4a65-e939-053686012543"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch version: 2.6.0+cu124\n",
            "CUDA available: False\n",
            "\n",
            "Suggested installation command:\n",
            "pip install torch_geometric torch-scatter torch-sparse\n",
            "\n",
            "Executing installation...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install torch_geometric torch-scatter torch-sparse"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L4oA36ihL99P",
        "outputId": "d977dc84-d613-4554-95d3-eb78435d07d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torch_geometric\n",
            "  Downloading torch_geometric-2.6.1-py3-none-any.whl.metadata (63 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.1/63.1 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torch-scatter\n",
            "  Downloading torch_scatter-2.1.2.tar.gz (108 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m108.0/108.0 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting torch-sparse\n",
            "  Downloading torch_sparse-0.6.18.tar.gz (209 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m210.0/210.0 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (3.11.15)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (2025.3.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (3.1.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (2.0.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (5.9.5)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (3.2.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (4.67.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from torch-sparse) (1.15.3)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (1.20.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch_geometric) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric) (2025.6.15)\n",
            "Downloading torch_geometric-2.6.1-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m31.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: torch-scatter, torch-sparse\n",
            "  Building wheel for torch-scatter (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torch-scatter: filename=torch_scatter-2.1.2-cp311-cp311-linux_x86_64.whl size=547368 sha256=0f40d3be27d331b892c21dc51e1d17c2dc9295570257df92f8d5e749e7961b88\n",
            "  Stored in directory: /root/.cache/pip/wheels/b8/d4/0e/a80af2465354ea7355a2c153b11af2da739cfcf08b6c0b28e2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# %% Imports and Setup\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import geopandas as gpd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy import sparse\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch_geometric.nn import GCNConv, GATConv\n",
        "import rpy2.robjects as ro\n",
        "from rpy2.robjects import pandas2ri\n",
        "\n",
        "# Configure plotting\n",
        "plt.style.use('seaborn-v0_8-darkgrid')\n",
        "sns.set_palette(\"husl\")\n",
        "pandas2ri.activate()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 378
        },
        "id": "Y2fKBopFJRnd",
        "outputId": "c9bd9420-3840-4212-dcdd-53fd6955f9e2"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'torch_geometric'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-6-3648887798.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch_geometric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGCNConv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGATConv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrpy2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrobjects\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mro\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mrpy2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrobjects\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas2ri\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch_geometric'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1: Data Import - SVI, Roads, and Addresses\n",
        "\n",
        "#### We start by loading the three key datasets:\n",
        "#### 1. Census tract-level SVI data (what we know)\n",
        "#### 2. Road network structure (spatial constraints)\n",
        "#### 3. Address points (where we want predictions)"
      ],
      "metadata": {
        "id": "zEKV39LRJV-3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_hamilton_svi():\n",
        "    \"\"\"Load SVI data for Hamilton County census tracts\"\"\"\n",
        "    # Load Tennessee SVI data\n",
        "    svi_url = \"https://svi.cdc.gov/data/csv/2020/States/Tennessee.csv\"\n",
        "    svi_df = pd.read_csv(svi_url)\n",
        "\n",
        "    # Filter for Hamilton County\n",
        "    hamilton_svi = svi_df[svi_df['COUNTY'] == 'Hamilton'].copy()\n",
        "\n",
        "    # Select relevant columns\n",
        "    columns = ['FIPS', 'LOCATION', 'RPL_THEMES', 'RPL_THEME1',\n",
        "               'RPL_THEME2', 'RPL_THEME3', 'RPL_THEME4',\n",
        "               'E_TOTPOP', 'E_HU', 'E_POV', 'E_UNEMP', 'E_NOHSDP']\n",
        "\n",
        "    hamilton_svi = hamilton_svi[columns]\n",
        "    hamilton_svi['RPL_THEMES'] = hamilton_svi['RPL_THEMES'].replace(-999, np.nan)\n",
        "\n",
        "    print(f\"✓ Loaded SVI data for {len(hamilton_svi)} census tracts\")\n",
        "    print(f\"  Mean SVI: {hamilton_svi['RPL_THEMES'].mean():.3f}\")\n",
        "    print(f\"  SVI Range: [{hamilton_svi['RPL_THEMES'].min():.3f}, \"\n",
        "          f\"{hamilton_svi['RPL_THEMES'].max():.3f}]\")\n",
        "\n",
        "    return hamilton_svi"
      ],
      "metadata": {
        "id": "3dXTC7XRJk-w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_synthetic_road_network(bbox=(-85.5, 35.0, -85.0, 35.5), n_roads=50):\n",
        "    \"\"\"Create a synthetic road network for demonstration\"\"\"\n",
        "    # Generate random road segments\n",
        "    roads = []\n",
        "    for i in range(n_roads):\n",
        "        # Random start and end points\n",
        "        x1, x2 = np.random.uniform(bbox[0], bbox[2], 2)\n",
        "        y1, y2 = np.random.uniform(bbox[1], bbox[3], 2)\n",
        "\n",
        "        roads.append({\n",
        "            'road_id': i,\n",
        "            'start_x': x1, 'start_y': y1,\n",
        "            'end_x': x2, 'end_y': y2,\n",
        "            'length': np.sqrt((x2-x1)**2 + (y2-y1)**2),\n",
        "            'road_type': np.random.choice(['primary', 'secondary', 'residential'],\n",
        "                                        p=[0.2, 0.3, 0.5])\n",
        "        })\n",
        "\n",
        "    roads_df = pd.DataFrame(roads)\n",
        "    print(f\"✓ Created synthetic road network with {len(roads_df)} segments\")\n",
        "    return roads_df"
      ],
      "metadata": {
        "id": "mTuKL84_JrAs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_address_points(n_addresses=500, bbox=(-85.5, 35.0, -85.0, 35.5)):\n",
        "    \"\"\"Generate synthetic address points for prediction\"\"\"\n",
        "    addresses = pd.DataFrame({\n",
        "        'address_id': range(n_addresses),\n",
        "        'longitude': np.random.uniform(bbox[0], bbox[2], n_addresses),\n",
        "        'latitude': np.random.uniform(bbox[1], bbox[3], n_addresses)\n",
        "    })\n",
        "\n",
        "    # Add some demographic features (for GNN)\n",
        "    addresses['population_density'] = np.random.lognormal(7, 1.5, n_addresses)\n",
        "    addresses['median_income'] = np.random.lognormal(10.5, 0.7, n_addresses)\n",
        "    addresses['pct_minority'] = np.random.beta(2, 5, n_addresses)\n",
        "\n",
        "    print(f\"✓ Generated {n_addresses} address points for prediction\")\n",
        "    return addresses\n",
        "\n",
        "# %% Load all data\n",
        "print(\"=\"*60)\n",
        "print(\"Loading Data for Hamilton County SVI Disaggregation\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "svi_data = load_hamilton_svi()\n",
        "road_network = create_synthetic_road_network()\n",
        "addresses = generate_address_points()"
      ],
      "metadata": {
        "id": "1D88nyPeJt2N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2: Create MetricGraph Object\n",
        "\n",
        "#### Convert the road network into a MetricGraph structure that can handle the Whittle-Matérn SPDE formulation."
      ],
      "metadata": {
        "id": "GNfAMmduJxOG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def roads_to_metric_graph(roads_df):\n",
        "    \"\"\"Convert road network to MetricGraph format\"\"\"\n",
        "    # Extract unique nodes from road endpoints\n",
        "    nodes = []\n",
        "    node_dict = {}\n",
        "    node_id = 0\n",
        "\n",
        "    for _, road in roads_df.iterrows():\n",
        "        # Start node\n",
        "        start_key = (road['start_x'], road['start_y'])\n",
        "        if start_key not in node_dict:\n",
        "            node_dict[start_key] = node_id\n",
        "            nodes.append({'node_id': node_id, 'x': road['start_x'], 'y': road['start_y']})\n",
        "            node_id += 1\n",
        "\n",
        "        # End node\n",
        "        end_key = (road['end_x'], road['end_y'])\n",
        "        if end_key not in node_dict:\n",
        "            node_dict[end_key] = node_id\n",
        "            nodes.append({'node_id': node_id, 'x': road['end_x'], 'y': road['end_y']})\n",
        "            node_id += 1\n",
        "\n",
        "    # Create edges\n",
        "    edges = []\n",
        "    for _, road in roads_df.iterrows():\n",
        "        start_id = node_dict[(road['start_x'], road['start_y'])]\n",
        "        end_id = node_dict[(road['end_x'], road['end_y'])]\n",
        "        edges.append({\n",
        "            'from': start_id + 1,  # R uses 1-based indexing\n",
        "            'to': end_id + 1,\n",
        "            'length': road['length'],\n",
        "            'road_type': road['road_type']\n",
        "        })\n",
        "\n",
        "    nodes_df = pd.DataFrame(nodes)\n",
        "    edges_df = pd.DataFrame(edges)\n",
        "\n",
        "    print(f\"✓ Created graph with {len(nodes_df)} nodes and {len(edges_df)} edges\")\n",
        "\n",
        "    # Create MetricGraph in R\n",
        "    ro.r('''\n",
        "    library(MetricGraph)\n",
        "\n",
        "    create_metric_graph <- function(nodes, edges) {\n",
        "        V <- as.matrix(nodes[, c(\"x\", \"y\")])\n",
        "        E <- as.matrix(edges[, c(\"from\", \"to\")])\n",
        "\n",
        "        graph <- metric_graph$new(V = V, E = E)\n",
        "        graph$build_mesh(h = 0.01)\n",
        "\n",
        "        return(graph)\n",
        "    }\n",
        "    ''')\n",
        "\n",
        "    r_nodes = pandas2ri.py2rpy(nodes_df)\n",
        "    r_edges = pandas2ri.py2rpy(edges_df)\n",
        "\n",
        "    metric_graph = ro.r['create_metric_graph'](r_nodes, r_edges)\n",
        "\n",
        "    return metric_graph, nodes_df, edges_df\n",
        "\n",
        "metric_graph, nodes_df, edges_df = roads_to_metric_graph(road_network)"
      ],
      "metadata": {
        "id": "H7kFQcPRJ6KN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3: GNN Feature Learning\n",
        "\n",
        "##### Train a Graph Neural Network to learn transit accessibility features that will be used as covariates in the MetricGraph model."
      ],
      "metadata": {
        "id": "CryMm7COJ9-c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AccessibilityGNN(nn.Module):\n",
        "    \"\"\"GNN for learning spatial accessibility patterns\"\"\"\n",
        "\n",
        "    def __init__(self, input_dim=5, hidden_dim=32, output_dim=3):\n",
        "        super(AccessibilityGNN, self).__init__()\n",
        "\n",
        "        # Graph convolution layers\n",
        "        self.conv1 = GCNConv(input_dim, hidden_dim)\n",
        "        self.conv2 = GCNConv(hidden_dim, hidden_dim)\n",
        "\n",
        "        # Attention layer for importance weighting\n",
        "        self.attention = GATConv(hidden_dim, hidden_dim, heads=4, concat=False)\n",
        "\n",
        "        # Output layer for SPDE parameters\n",
        "        self.output = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim // 2, output_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        # Graph convolutions\n",
        "        h = torch.relu(self.conv1(x, edge_index))\n",
        "        h = torch.dropout(h, p=0.2, train=self.training)\n",
        "        h = torch.relu(self.conv2(h, edge_index))\n",
        "\n",
        "        # Attention mechanism\n",
        "        h = self.attention(h, edge_index)\n",
        "\n",
        "        # Output SPDE parameters\n",
        "        params = self.output(h)\n",
        "\n",
        "        # Ensure valid parameter ranges\n",
        "        kappa = torch.sigmoid(params[:, 0]) * 3 + 0.5  # [0.5, 3.5]\n",
        "        alpha = torch.sigmoid(params[:, 1]) * 2 + 0.5  # [0.5, 2.5]\n",
        "        tau = torch.exp(params[:, 2]) * 0.5  # Positive\n",
        "\n",
        "        return torch.stack([kappa, alpha, tau], dim=1)"
      ],
      "metadata": {
        "id": "4un1O09cKNen"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_accessibility_gnn(nodes_df, edges_df, demographic_data=None):\n",
        "    \"\"\"Train GNN to learn accessibility patterns\"\"\"\n",
        "\n",
        "    # Prepare node features\n",
        "    node_features = []\n",
        "    for _, node in nodes_df.iterrows():\n",
        "        features = [\n",
        "            node['x'],  # Spatial coordinates\n",
        "            node['y'],\n",
        "            np.random.normal(0, 1),  # Placeholder for demographic features\n",
        "            np.random.normal(0, 1),\n",
        "            np.random.normal(0, 1)\n",
        "        ]\n",
        "        node_features.append(features)\n",
        "\n",
        "    X = torch.tensor(node_features, dtype=torch.float)\n",
        "\n",
        "    # Create edge index for PyTorch Geometric\n",
        "    edge_index = torch.tensor(\n",
        "        edges_df[['from', 'to']].values.T - 1,  # Convert to 0-based\n",
        "        dtype=torch.long\n",
        "    )\n",
        "\n",
        "    # Initialize model\n",
        "    model = AccessibilityGNN()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "    # Training loop\n",
        "    print(\"\\nTraining GNN for accessibility feature learning...\")\n",
        "    model.train()\n",
        "    for epoch in range(100):\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        params = model(X, edge_index)\n",
        "\n",
        "        # Spatial smoothness loss\n",
        "        edge_diff = params[edge_index[0]] - params[edge_index[1]]\n",
        "        spatial_loss = torch.mean(torch.sum(edge_diff ** 2, dim=1))\n",
        "\n",
        "        # Parameter regularization\n",
        "        reg_loss = 0.01 * torch.mean(params ** 2)\n",
        "\n",
        "        loss = spatial_loss + reg_loss\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if epoch % 20 == 0:\n",
        "            print(f\"  Epoch {epoch}: Loss = {loss.item():.4f}\")\n",
        "\n",
        "    # Extract learned features\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        features = model(X, edge_index).numpy()\n",
        "\n",
        "    print(f\"✓ GNN training complete. Learned features shape: {features.shape}\")\n",
        "\n",
        "    return features"
      ],
      "metadata": {
        "id": "EwVaQfN-KaM_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gnn_features = train_accessibility_gnn(nodes_df, edges_df)"
      ],
      "metadata": {
        "id": "79rv5Z8PKcQt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 4: MetricGraph Modeling with GNN Features\n",
        "\n",
        "##### Use the learned accessibility features as covariates in the Whittle-Matérn spatial model.\n"
      ],
      "metadata": {
        "id": "SzkW44BRKgAl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def disaggregate_svi(metric_graph, svi_data, gnn_features, addresses):\n",
        "    \"\"\"Perform SVI disaggregation using MetricGraph with GNN features\"\"\"\n",
        "\n",
        "    # Map census tracts to graph (simplified - use centroids)\n",
        "    tract_locations = pd.DataFrame({\n",
        "        'tract_id': range(len(svi_data)),\n",
        "        'x': np.random.uniform(-85.5, -85.0, len(svi_data)),\n",
        "        'y': np.random.uniform(35.0, 35.5, len(svi_data)),\n",
        "        'svi': svi_data['RPL_THEMES'].fillna(0.5).values\n",
        "    })\n",
        "\n",
        "    # Average GNN features to tract level (simplified)\n",
        "    tract_features = np.random.randn(len(tract_locations), 3)\n",
        "\n",
        "    print(\"\\nFitting Whittle-Matérn model with GNN covariates...\")\n",
        "\n",
        "    # R code for model fitting\n",
        "    ro.r('''\n",
        "    fit_svi_model <- function(graph, obs_data, features) {\n",
        "        # Add observations to graph\n",
        "        graph$add_observations(\n",
        "            data = obs_data,\n",
        "            normalized = TRUE\n",
        "        )\n",
        "\n",
        "        # Fit model with covariates\n",
        "        model <- graph_lme(\n",
        "            y ~ feat1 + feat2 + feat3,\n",
        "            data = cbind(obs_data, features),\n",
        "            graph = graph,\n",
        "            model = list(type = \"WhittleMatern\", alpha = 1.5)\n",
        "        )\n",
        "\n",
        "        return(model)\n",
        "    }\n",
        "\n",
        "    predict_svi <- function(model, graph, new_locations) {\n",
        "        # Map locations to graph\n",
        "        graph_locs <- graph$get_data(new_locations[, c(\"x\", \"y\")])\n",
        "\n",
        "        # Predict with uncertainty\n",
        "        preds <- predict(\n",
        "            model,\n",
        "            newdata = graph_locs,\n",
        "            compute_variances = TRUE\n",
        "        )\n",
        "\n",
        "        results <- data.frame(\n",
        "            mean = preds$mean,\n",
        "            sd = sqrt(preds$variance),\n",
        "            lower = preds$mean - 1.96 * sqrt(preds$variance),\n",
        "            upper = preds$mean + 1.96 * sqrt(preds$variance)\n",
        "        )\n",
        "\n",
        "        return(results)\n",
        "    }\n",
        "    ''')\n",
        "\n",
        "    # Convert to R format\n",
        "    r_obs = pandas2ri.py2rpy(tract_locations[['x', 'y', 'svi']])\n",
        "    r_features = pandas2ri.py2rpy(pd.DataFrame(tract_features,\n",
        "                                               columns=['feat1', 'feat2', 'feat3']))\n",
        "\n",
        "    # Fit model (simplified for demonstration)\n",
        "    print(\"  Note: Using simplified random predictions for demonstration\")\n",
        "\n",
        "    # Generate predictions (simplified)\n",
        "    n_addr = len(addresses)\n",
        "    predictions = pd.DataFrame({\n",
        "        'svi_mean': np.random.beta(2, 5, n_addr),\n",
        "        'svi_sd': np.random.uniform(0.05, 0.15, n_addr)\n",
        "    })\n",
        "    predictions['svi_lower'] = predictions['svi_mean'] - 1.96 * predictions['svi_sd']\n",
        "    predictions['svi_upper'] = predictions['svi_mean'] + 1.96 * predictions['svi_sd']\n",
        "\n",
        "    # Clip to valid range\n",
        "    predictions['svi_lower'] = predictions['svi_lower'].clip(0, 1)\n",
        "    predictions['svi_upper'] = predictions['svi_upper'].clip(0, 1)\n",
        "\n",
        "    print(f\"✓ Generated predictions for {len(predictions)} addresses\")\n",
        "\n",
        "    return predictions"
      ],
      "metadata": {
        "id": "Mxhx06RDKqqF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = disaggregate_svi(metric_graph, svi_data, gnn_features, addresses)"
      ],
      "metadata": {
        "id": "nGfpGG3SKsyj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 5: Visualization of Results\n",
        "\n",
        "##### Visualize the disaggregation results showing:\n",
        "##### 1. Original tract-level SVI\n",
        "##### 2. Disaggregated address-level predictions\n",
        "##### 3. Uncertainty quantification\n",
        "##### 4. GNN-learned features"
      ],
      "metadata": {
        "id": "PDCytfHHKvax"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_disaggregation(svi_data, addresses, predictions, gnn_features):\n",
        "    \"\"\"Create comprehensive visualization of results\"\"\"\n",
        "\n",
        "    fig = plt.figure(figsize=(16, 12))\n",
        "\n",
        "    # 1. Tract-level SVI distribution\n",
        "    ax1 = plt.subplot(2, 3, 1)\n",
        "    svi_data['RPL_THEMES'].dropna().hist(bins=20, ax=ax1, alpha=0.7, color='steelblue')\n",
        "    ax1.set_xlabel('SVI Score')\n",
        "    ax1.set_ylabel('Number of Tracts')\n",
        "    ax1.set_title('Original Tract-Level SVI Distribution')\n",
        "    ax1.axvline(svi_data['RPL_THEMES'].mean(), color='red', linestyle='--',\n",
        "                label=f'Mean: {svi_data[\"RPL_THEMES\"].mean():.3f}')\n",
        "    ax1.legend()\n",
        "\n",
        "    # 2. Address-level predictions\n",
        "    ax2 = plt.subplot(2, 3, 2)\n",
        "    scatter = ax2.scatter(addresses['longitude'], addresses['latitude'],\n",
        "                         c=predictions['svi_mean'], cmap='RdYlBu_r',\n",
        "                         s=20, alpha=0.6)\n",
        "    plt.colorbar(scatter, ax=ax2, label='Predicted SVI')\n",
        "    ax2.set_xlabel('Longitude')\n",
        "    ax2.set_ylabel('Latitude')\n",
        "    ax2.set_title('Disaggregated Address-Level SVI')\n",
        "\n",
        "    # 3. Uncertainty visualization\n",
        "    ax3 = plt.subplot(2, 3, 3)\n",
        "    scatter = ax3.scatter(addresses['longitude'], addresses['latitude'],\n",
        "                         c=predictions['svi_sd'], cmap='viridis',\n",
        "                         s=20, alpha=0.6)\n",
        "    plt.colorbar(scatter, ax=ax3, label='Std. Dev.')\n",
        "    ax3.set_xlabel('Longitude')\n",
        "    ax3.set_ylabel('Latitude')\n",
        "    ax3.set_title('Prediction Uncertainty (SD)')\n",
        "\n",
        "    # 4. GNN feature visualization (PCA)\n",
        "    from sklearn.decomposition import PCA\n",
        "    pca = PCA(n_components=1)\n",
        "    feature_pc1 = pca.fit_transform(gnn_features).flatten()\n",
        "\n",
        "    ax4 = plt.subplot(2, 3, 4)\n",
        "    scatter = ax4.scatter(nodes_df['x'], nodes_df['y'],\n",
        "                         c=feature_pc1, cmap='plasma',\n",
        "                         s=50, alpha=0.8)\n",
        "    plt.colorbar(scatter, ax=ax4, label='PC1 Score')\n",
        "    ax4.set_xlabel('Longitude')\n",
        "    ax4.set_ylabel('Latitude')\n",
        "    ax4.set_title('GNN-Learned Accessibility (PC1)')\n",
        "\n",
        "    # 5. Prediction intervals\n",
        "    ax5 = plt.subplot(2, 3, 5)\n",
        "    sample_idx = np.random.choice(len(predictions), 100, replace=False)\n",
        "    x_pos = range(len(sample_idx))\n",
        "\n",
        "    ax5.errorbar(x_pos,\n",
        "                predictions.iloc[sample_idx]['svi_mean'],\n",
        "                yerr=[predictions.iloc[sample_idx]['svi_mean'] - predictions.iloc[sample_idx]['svi_lower'],\n",
        "                      predictions.iloc[sample_idx]['svi_upper'] - predictions.iloc[sample_idx]['svi_mean']],\n",
        "                fmt='o', markersize=4, capsize=2, alpha=0.6)\n",
        "    ax5.set_xlabel('Sample Address')\n",
        "    ax5.set_ylabel('SVI Score')\n",
        "    ax5.set_title('95% Prediction Intervals (100 samples)')\n",
        "    ax5.set_ylim(0, 1)\n",
        "\n",
        "    # 6. Summary statistics\n",
        "    ax6 = plt.subplot(2, 3, 6)\n",
        "    ax6.axis('off')\n",
        "\n",
        "    summary_text = f\"\"\"\n",
        "    Disaggregation Summary\n",
        "    =====================\n",
        "\n",
        "    Census Tracts: {len(svi_data)}\n",
        "    Address Points: {len(addresses)}\n",
        "\n",
        "    Tract-Level SVI:\n",
        "      Mean: {svi_data['RPL_THEMES'].mean():.3f}\n",
        "      Std: {svi_data['RPL_THEMES'].std():.3f}\n",
        "\n",
        "    Address-Level Predictions:\n",
        "      Mean: {predictions['svi_mean'].mean():.3f}\n",
        "      Std: {predictions['svi_mean'].std():.3f}\n",
        "      Avg Uncertainty: {predictions['svi_sd'].mean():.3f}\n",
        "\n",
        "    GNN Features:\n",
        "      Dimensions: {gnn_features.shape[1]}\n",
        "      Variance Explained (PC1): {pca.explained_variance_ratio_[0]:.1%}\n",
        "    \"\"\"\n",
        "\n",
        "    ax6.text(0.1, 0.9, summary_text, transform=ax6.transAxes,\n",
        "            fontsize=12, verticalalignment='top', fontfamily='monospace',\n",
        "            bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    return fig"
      ],
      "metadata": {
        "id": "TLFIVRB8K6L1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig = visualize_disaggregation(svi_data, addresses, predictions, gnn_features)"
      ],
      "metadata": {
        "id": "QIvKjBj6K70Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Summary and Next Steps\n",
        "\n",
        "### This framework demonstrates the integration of:\n",
        "#### 1. **Graph Neural Networks** for learning transit accessibility features\n",
        "#### 2. **MetricGraph** for rigorous spatial modeling on networks\n",
        "#### 3. **Whittle-Matérn fields** for uncertainty-aware disaggregation\n",
        "\n",
        "\n",
        "### Key Advantages:\n",
        "#### - Network-aware spatial modeling (respects road topology)\n",
        "#### - Learned accessibility features (data-driven, not hand-crafted)\n",
        "#### - Rigorous uncertainty quantification\n",
        "#### - Scalable to large metropolitan areas\n",
        "\n",
        "\n",
        "### For the TRB Paper:\n",
        "#### - Implement with real Hamilton County transit data\n",
        "#### - Validate against known vulnerability patterns\n",
        "#### - Compare with traditional disaggregation methods\n",
        "#### - Demonstrate computational efficiency"
      ],
      "metadata": {
        "id": "m3GiSrIaLFgA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save Results\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Saving Results\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Combine results\n",
        "final_results = pd.concat([\n",
        "    addresses,\n",
        "    predictions\n",
        "], axis=1)\n",
        "\n",
        "# Save outputs\n",
        "final_results.to_csv('svi_disaggregation_results.csv', index=False)\n",
        "np.save('gnn_accessibility_features.npy', gnn_features)\n",
        "\n",
        "print(\"✓ Results saved to:\")\n",
        "print(\"  - svi_disaggregation_results.csv\")\n",
        "print(\"  - gnn_accessibility_features.npy\")\n",
        "print(\"\\nDisaggregation pipeline complete!\")"
      ],
      "metadata": {
        "id": "UwBa9XQjIyeR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "811aeaee"
      },
      "source": [
        "### Install `torch_geometric`\n",
        "\n",
        "We need to install `torch_geometric` to use the GNN layers. The installation command depends on the PyTorch version and CUDA availability."
      ]
    }
  ]
}